{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "#make environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "# as the environment is continues there cannot be finite number of states \n",
    "states = env.observation_space.n #used if discrete environment\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrozenLake-v0\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "The actions you can take are up - down - right - late\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of actions that can be \n",
    "actions = env.action_space.n\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize value table randomly\n",
    "value_table = np.zeros((states,1))\n",
    "value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iterations(env , n_iterations , gamma = 1.0 , threshold = 1e-30):\n",
    "    \"\"\"\n",
    "    n_iterations is gives the app number if times to train\n",
    "    gamma is \"discount-factor\" if the step should be taken or not\n",
    "    threshold is allowed error\n",
    "    this iterates through possible movements. Then creates table suggesting values\n",
    "    of reward we can get for being in a specific state. \n",
    "  \"\"\"\n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        new_valuetable = np.copy(value_table)\n",
    "        for state in range(states):\n",
    "            q_value = []\n",
    "            for action in range(actions):\n",
    "                next_state_reward = []\n",
    "                for next_state_parameters in env.env.P[state][action]:\n",
    "                    transition_prob, next_state, reward_prob, _ = next_state_parameters\n",
    "                    reward = transition_prob*(reward_prob+gamma*new_valuetable[next_state])\n",
    "                    next_state_reward.append(reward)\n",
    "                    \n",
    "                    \n",
    "                q_value.append((np.sum(next_state_reward)))\n",
    "            value_table[state] = max(q_value)\n",
    "            \n",
    "        if (np.sum(np.fabs(new_valuetable - value_table))<=threshold):\n",
    "            break\n",
    "    return value_table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(value_table, gamma = 1.0):\n",
    "    \"\"\" \n",
    "    policy is the definition of what action to take \n",
    "    \"\"\"\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for state in range(env.observation_space.n):\n",
    "        Q_table = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            for next_sr in env.env.P[state][action]:\n",
    "                transition_prob, next_state, reward_prob, _ = next_sr\n",
    "                Q_table[action] += (transition_prob * (reward_prob + gamma *value_table[next_state]))\n",
    "        policy[state] = np.argmax(Q_table)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82352941],\n",
       "       [0.82352941],\n",
       "       [0.82352941],\n",
       "       [0.82352941],\n",
       "       [0.82352941],\n",
       "       [0.        ],\n",
       "       [0.52941176],\n",
       "       [0.        ],\n",
       "       [0.82352941],\n",
       "       [0.82352941],\n",
       "       [0.76470588],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.88235294],\n",
       "       [0.94117647],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_table = value_iterations(env,10000)\n",
    "value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "policy = extract_policy(value_table)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
